import os
import time
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import models, transforms

# === EXPLANATION ===
# This script trains and validates a ResNet-18 image classification model using
# image paths and labels stored in CSV files. It provides a complete training
# workflow optimized for macOS MPS (Metal) acceleration and supports resuming
# from checkpoints for long-running training sessions.
#
# Major components:
#   • CSVImageDataset class:
#         - Loads file paths and labels from CSVs
#         - Validates image existence and filters missing files
#         - Converts class names to integer indices with a shared mapping
#         - Applies user-defined torchvision transforms
#
#   • Training pipeline (train_model):
#         - Uses mixed precision automatically when running on MPS
#         - Tracks loss and accuracy for both train and validation sets
#         - Saves a checkpoint after every epoch (model, optimizer, scaler, epoch)
#         - Automatically saves the best model (highest val accuracy) as
#               'best_mushroom_model.pth'
#         - Supports resuming interrupted training from checkpoint.pth
#
#   • Model setup:
#         - Loads pretrained ResNet-18
#         - Freezes the backbone (feature extractor)
#         - Replaces the final layer with a new classifier head sized to the
#           number of discovered classes
#         - Optionally tries torch.compile() (falls back safely on failure)
#
#   • Dataset and dataloaders:
#         - Train/val CSVs provide image_path and label columns
#         - Training uses augmentation (random crop + flip)
#         - Validation uses deterministic resize + center crop
#
#   • Utilities:
#         - plot_history(): saves + displays training/validation curves
#         - Prints device info, dataset sizes, and class count
#
# Outputs generated by this script:
#   • best_mushroom_model.pth       – best performing weights
#   • checkpoint.pth               – full training state for resuming
#   • training_history.png         – loss/accuracy curves
#
# This script is designed to be a lightweight, robust baseline for training
# CNN image classifiers from CSV metadata on macOS/MPS devices but is easily
# adaptable to other datasets or hardware setups.

# --- MPS / macOS specific optimization ---
torch.backends.mps.benchmark = True  # speed up MPS kernels


class CSVImageDataset(Dataset):
    def __init__(self, csv_file, root_dir, transform=None, class_to_idx=None):
        self.root_dir = root_dir
        self.transform = transform
        annotations = pd.read_csv(csv_file)

        if (
            "image_path" not in annotations.columns
            or "label" not in annotations.columns
        ):
            raise ValueError("CSV must contain columns 'image_path' and 'label'")

        def _full_path(p):
            return os.path.join(self.root_dir, str(p).lstrip("/"))

        annotations["full_path"] = annotations["image_path"].apply(_full_path)

        missing_mask = ~annotations["full_path"].apply(os.path.exists)
        n_missing = missing_mask.sum()
        if n_missing > 0:
            print(f"WARNING: {n_missing} images not found; they will be ignored.")
            annotations = annotations[~missing_mask].reset_index(drop=True)

        self.annotations = annotations
        if class_to_idx is None:
            self.classes = sorted(annotations["label"].unique())
            self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}
        else:
            self.class_to_idx = class_to_idx
            self.classes = sorted(class_to_idx.keys())

        self.annotations["label_idx"] = self.annotations["label"].map(self.class_to_idx)
        if self.annotations["label_idx"].isnull().any():
            raise ValueError(
                "Some labels in CSV are not present in class_to_idx mapping."
            )

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        row = self.annotations.iloc[idx]
        image = Image.open(row["full_path"]).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, int(row["label_idx"])


def train_model(
    model,
    criterion,
    optimizer,
    dataloaders,
    device,
    num_epochs=25,
    start_epoch=0,
    best_acc=0.0,
    checkpoint_path="checkpoint.pth",
    scaler_state_dict=None,
):
    since = time.time()
    best_model_wts = model.state_dict()  # keep reference, avoid deepcopy on MPS
    use_mps = device.type == "mps"
    scaler = torch.amp.GradScaler(enabled=use_mps)

    if scaler_state_dict:
        try:
            scaler.load_state_dict(scaler_state_dict)
            print("Loaded scaler state.")
        except Exception as e:
            print(f"Warning: could not load scaler state: {e}")

    history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}

    for epoch in range(start_epoch, num_epochs):
        print(f"Epoch {epoch + 1}/{num_epochs}\n" + "-" * 10)
        for phase in ["train", "val"]:
            model.train() if phase == "train" else model.eval()
            running_loss, running_corrects = 0.0, 0
            dataset_size = len(dataloaders[phase].dataset)
            if dataset_size == 0:
                print(f"{phase} dataset empty, skipping.")
                history[f"{phase}_loss"].append(0.0)
                history[f"{phase}_acc"].append(0.0)
                continue

            loop = tqdm(dataloaders[phase], desc=phase, leave=False)
            for inputs, labels in loop:
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer.zero_grad()

                with torch.amp.autocast(device_type=device.type, enabled=use_mps):
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)

                _, preds = torch.max(outputs, 1)
                if phase == "train":
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data).item()

            epoch_loss = running_loss / dataset_size
            epoch_acc = running_corrects / dataset_size
            history[f"{phase}_loss"].append(epoch_loss)
            history[f"{phase}_acc"].append(epoch_acc)

            print(f"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}")

            if phase == "val" and epoch_acc > best_acc:
                best_acc = epoch_acc
                torch.save(model.state_dict(), "best_mushroom_model.pth")
                print("Saved new best model!")

        checkpoint = {
            "epoch": epoch + 1,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scaler_state_dict": scaler.state_dict(),
            "best_acc": best_acc,
        }
        torch.save(checkpoint, checkpoint_path)
        print(f"Checkpoint saved to {checkpoint_path}\n")

    print(
        f"Training complete in {int((time.time() - since) // 60)}m {(time.time() - since) % 60:.0f}s"
    )
    print(f"Best val Acc: {best_acc:.4f}")
    model.load_state_dict(torch.load("best_mushroom_model.pth"))
    return model, history


def plot_history(history, out_path="training_history.png"):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    ax1.plot(history["train_loss"], label="Train Loss")
    ax1.plot(history["val_loss"], label="Val Loss")
    ax1.set_title("Loss History")
    ax1.set_xlabel("Epoch")
    ax1.set_ylabel("Loss")
    ax1.legend()
    ax2.plot(history["train_acc"], label="Train Acc")
    ax2.plot(history["val_acc"], label="Val Acc")
    ax2.set_title("Accuracy History")
    ax2.set_xlabel("Epoch")
    ax2.set_ylabel("Accuracy")
    ax2.legend()
    plt.tight_layout()
    plt.savefig(out_path)
    plt.show()


if __name__ == "__main__":
    DATA_ROOT = "."
    TRAIN_CSV, VAL_CSV = "train.csv", "val.csv"
    NUM_EPOCHS, BATCH_SIZE = 15, 128
    CHECKPOINT_PATH = "checkpoint.pth"

    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print(f"Using device: {device}")

    data_transforms = {
        "train": transforms.Compose(
            [
                transforms.RandomResizedCrop(224),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
            ]
        ),
        "val": transforms.Compose(
            [
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
            ]
        ),
    }

    train_dataset = CSVImageDataset(TRAIN_CSV, DATA_ROOT, data_transforms["train"])
    class_to_idx = train_dataset.class_to_idx
    val_dataset = CSVImageDataset(
        VAL_CSV, DATA_ROOT, data_transforms["val"], class_to_idx
    )

    dataloaders = {
        "train": DataLoader(
            train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4
        ),
        "val": DataLoader(
            val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4
        ),
    }

    print(
        f"Found {len(class_to_idx)} classes. Training on {len(train_dataset)}, validating on {len(val_dataset)} images."
    )

    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
    for param in model.parameters():
        param.requires_grad = False
    model.fc = nn.Linear(model.fc.in_features, len(class_to_idx))
    model = model.to(device)

    try:
        model = torch.compile(model)
    except:
        pass  # torch.compile may not be fully supported on MPS

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.fc.parameters(), lr=0.001)

    start_epoch, best_acc, scaler_state_dict = 0, 0.0, None
    if os.path.exists(CHECKPOINT_PATH):
        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)
        model.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        start_epoch = checkpoint.get("epoch", 0)
        best_acc = checkpoint.get("best_acc", 0.0)
        scaler_state_dict = checkpoint.get("scaler_state_dict", None)
        print(f"Resuming from epoch {start_epoch} with best acc {best_acc:.4f}")

    trained_model, history = train_model(
        model,
        criterion,
        optimizer,
        dataloaders,
        device,
        num_epochs=NUM_EPOCHS,
        start_epoch=start_epoch,
        best_acc=best_acc,
        checkpoint_path=CHECKPOINT_PATH,
        scaler_state_dict=scaler_state_dict,
    )

    plot_history(history)
    print("Training finished. Best model saved to 'best_mushroom_model.pth'")
